{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755d3173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 11:13:54,968 - api.services.database_service - INFO - Connected to MongoDB database: healthcare\n",
      "2025-07-25 11:13:55,093 - src.utils.notebook_setup - INFO - Database connected: True\n",
      "2025-07-25 11:13:55,094 - src.utils.notebook_setup - INFO - Database collections: ['heart_disease_gold', 'heart_disease_silver', 'heart_disease_bronze']\n",
      "2025-07-25 11:13:55,096 - src.utils.notebook_setup - INFO - Database collections count: 3\n",
      "2025-07-25 11:13:55,098 - src.utils.notebook_setup - INFO - === STARTING MODEL EVALUATION ===\n",
      "2025-07-25 11:13:55,968 - src.utils.training_utils - INFO - Training set: (15456, 16), Test set: (3864, 16)\n",
      "2025-07-25 11:13:55,971 - src.utils.notebook_setup - INFO - Loaded test set: (3864, 16)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import load as load_model\n",
    "import os\n",
    "from src.utils.evaluate_utils import evaluate_model, load_trained_models, clear_worse_models\n",
    "from src.utils.training_utils import prepare_data\n",
    "from src.utils.data_utils import drop_id\n",
    "import datetime\n",
    "\n",
    "# Quick setup\n",
    "from src.utils.notebook_setup import setup_notebook_environment\n",
    "dbs, logger = await setup_notebook_environment()\n",
    "\n",
    "logger.info(\"=== STARTING MODEL EVALUATION ===\")\n",
    "\n",
    "\n",
    "# LOAD DATA (same as 04)\n",
    "gold_data_from_db = await dbs.get_gold_data()\n",
    "gold_data_df = pd.DataFrame(gold_data_from_db)\n",
    "gold_data_df = drop_id(gold_data_df)\n",
    "\n",
    "# RECREATE THE SAME SPLIT (important!)\n",
    "X = gold_data_df.drop('target', axis=1)\n",
    "y = gold_data_df['target']\n",
    "X_train, X_test, y_train, y_test = prepare_data(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logger.info(f\"Loaded test set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca6607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 11:13:55,991 - src.utils.evaluate_utils - INFO - Loaded model: LogisticRegression\n",
      "2025-07-25 11:13:55,993 - src.utils.notebook_setup - INFO - Loaded 1 trained models\n"
     ]
    }
   ],
   "source": [
    "# load the trained models\n",
    "trained_models = load_trained_models()\n",
    "logger.info(f\"Loaded {len(trained_models)} trained models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e92b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 11:13:56,015 - src.utils.notebook_setup - INFO - Evaluating LogisticRegression...\n",
      "2025-07-25 11:13:56,061 - src.utils.evaluate_utils - INFO - \n",
      "LogisticRegression Evaluation Results:\n",
      "2025-07-25 11:13:56,063 - src.utils.evaluate_utils - INFO - Accuracy: 0.8080\n",
      "2025-07-25 11:13:56,065 - src.utils.evaluate_utils - INFO - Precision: 0.8105\n",
      "2025-07-25 11:13:56,069 - src.utils.evaluate_utils - INFO - Recall: 0.8522\n",
      "2025-07-25 11:13:56,071 - src.utils.evaluate_utils - INFO - F1-Score: 0.8308\n",
      "2025-07-25 11:13:56,073 - src.utils.evaluate_utils - INFO - ROC-AUC: 0.8877\n",
      "2025-07-25 11:13:56,078 - src.utils.notebook_setup - INFO - \n",
      "=== MODEL COMPARISON ===\n",
      "2025-07-25 11:13:56,082 - src.utils.notebook_setup - INFO -            model_name  accuracy  precision  recall  f1_score  roc_auc\n",
      "0  LogisticRegression     0.808     0.8105  0.8522    0.8308   0.8877\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ALL TRAINED MODELS\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    logger.info(f\"Evaluating {model_name}...\")\n",
    "    result = evaluate_model(model, X_test, y_test, model_name)\n",
    "    all_results.append(result)\n",
    "\n",
    "# Create results DataFrame\n",
    "all_results_df = pd.DataFrame([\n",
    "    {k: v for k, v in result.items() if k != 'model'} \n",
    "    for result in all_results\n",
    "])\n",
    "\n",
    "logger.info(\"\\n=== MODEL COMPARISON ===\")\n",
    "logger.info(all_results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec732dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 11:13:56,165 - src.utils.notebook_setup - INFO - \n",
      "=== BEST MODEL: LogisticRegression ===\n",
      "2025-07-25 11:13:56,169 - src.utils.notebook_setup - INFO - ROC-AUC: 0.8877\n"
     ]
    }
   ],
   "source": [
    "# FIND BEST ONE\n",
    "best_model_idx = all_results_df['roc_auc'].idxmax()\n",
    "best_model_name = all_results_df.iloc[best_model_idx]['model_name']\n",
    "best_model = all_results[best_model_idx]['model']\n",
    "\n",
    "\n",
    "\n",
    "logger.info(f\"\\n=== BEST MODEL: {best_model_name} ===\")\n",
    "logger.info(f\"ROC-AUC: {all_results_df.iloc[best_model_idx]['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1659d2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 11:13:56,280 - src.utils.evaluate_utils - INFO - Removing worse model: LogisticRegression_tuned.joblib\n",
      "2025-07-25 11:13:56,288 - src.utils.notebook_setup - INFO - SAVING MODEL METADATA\n",
      "2025-07-25 11:13:56,293 - src.utils.notebook_setup - INFO - Saved final model: LogisticRegression as heart_disease_classifier.joblib\n"
     ]
    }
   ],
   "source": [
    "# SAVE BEST MODEL TO DISK!\n",
    "from joblib import dump as dump_model\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Clear worse models\n",
    "clear_worse_models(best_model_name)\n",
    "\n",
    "# Save the winner as the final production model\n",
    "dump_model(best_model, \"../models/heart_disease_classifier.joblib\")\n",
    "\n",
    "best_result = all_results_df.iloc[best_model_idx]\n",
    "\n",
    "\n",
    "# Construct metadata dictionary\n",
    "model_metadata = {\n",
    "    \"model_name\": best_model_name,\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"created_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"algorithm\": type(best_model).__name__,\n",
    "    \"features\": list(X.columns),\n",
    "    \"target\": \"target\",\n",
    "    \"metrics\": {\n",
    "        \"accuracy\": best_result[\"accuracy\"],\n",
    "        \"precision\": best_result[\"precision\"],\n",
    "        \"recall\": best_result[\"recall\"],\n",
    "        \"f1_score\": best_result[\"f1_score\"],\n",
    "        \"roc_auc\": best_result[\"roc_auc\"]\n",
    "    },\n",
    "    \"training_data_size\": len(X_train),\n",
    "    \"test_data_size\": len(X_test)\n",
    "}\n",
    "\n",
    "# Save to model_metadata.json\n",
    "logger.info(\"SAVING MODEL METADATA\")   \n",
    "with open(\"../models/model_metadata.json\", \"w\") as f:\n",
    "    json.dump(model_metadata, f, indent=4)\n",
    "\n",
    "\n",
    "logger.info(f\"Saved final model: {best_model_name} as heart_disease_classifier.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
