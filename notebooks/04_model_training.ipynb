{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e6db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from src.utils.data_utils import drop_id\n",
    "from src.utils.training_utils import (\n",
    "    prepare_data,\n",
    "    train_model,\n",
    "    hyperparameter_tuning,\n",
    "    save_model,\n",
    "    remove_old_models,\n",
    "    get_models,\n",
    ")\n",
    "from src.utils.notebook_setup import setup_notebook_environment\n",
    "\n",
    "# Quick setup\n",
    "dbs, logger = await setup_notebook_environment()\n",
    "\n",
    "# Now ready to work\n",
    "logger.info(\"=== STARTING MODEL TRAINING ===\")\n",
    "\n",
    "# Checking Gold Layer\n",
    "gold_data_from_db = await dbs.get_gold_data()\n",
    "gold_data_df = pd.DataFrame(gold_data_from_db)\n",
    "gold_data_df = drop_id(gold_data_df)\n",
    "\n",
    "gold_data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be59a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARATE features and target\n",
    "X = gold_data_df.drop('target', axis=1)  # Features only (18 columns)\n",
    "y = gold_data_df['target']               # Target only (0s and 1s)\n",
    "\n",
    "# Then do train/test split\n",
    "X_train, X_test, y_train, y_test = prepare_data(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "logger.info(f\"X_train shape: {X_train.shape}\")\n",
    "logger.info(f\"X_test shape: {X_test.shape}\")\n",
    "logger.info(f\"y_train shape: {y_train.shape}\")\n",
    "logger.info(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6eeb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "# models = {\n",
    "#     'LogisticRegression'  : LogisticRegression(max_iter=1000),\n",
    "#     'RandomForest'        : RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "#     'XGBoost'              : XGBClassifier(objective='binary:logistic', n_estimators=100, learning_rate=0.1, random_state=42, scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train)),\n",
    "#     'DecisionTree'        : DecisionTreeClassifier(max_depth=5, random_state=42, class_weight='balanced'),\n",
    "# }\n",
    "\n",
    "models  = get_models(y_train)\n",
    "\n",
    "pprint(models, indent=4) # Use 4 spaces for inden\n",
    "pprint(models, width=40) # Set max line width\n",
    "\n",
    "logger.info(pprint(models))\n",
    "\n",
    "# Shows ALL available parameters\n",
    "logger.info(pprint(f\" Logistic Regression Parameters: {models['LogisticRegression'].get_params()}\") ) \n",
    "# logger.info(pprint(f\" Random Forest Parameters: {models['RandomForest'].get_params()}\"))\n",
    "# logger.info(pprint(f\" XGBoost Parameters: {models['XGBoost'].get_params()}\"))\n",
    "# logger.info(pprint(f\" Decision Tree Parameters: {models['DecisionTree'].get_params()}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35bb93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the models\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "remove_old_models()\n",
    "# Train models with correct parameter grids\n",
    "for name, model in models.items():\n",
    "    logger.info(f\"Training {name}...\")\n",
    "    \n",
    "    # Train basic model first\n",
    "    model = train_model(model, X_train, y_train, name)\n",
    "    \n",
    "    # Get the correct parameter grid for this model\n",
    "    if name in param_grids:\n",
    "        param_grid = param_grids[name]\n",
    "        logger.info(f\"Tuning hyperparameters for {name}...\")\n",
    "        model = hyperparameter_tuning(model, X_train, y_train, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    else:\n",
    "        logger.info(f\"No hyperparameter tuning for {name}\")\n",
    "    \n",
    "    # Save the model\n",
    "    save_model(model, name) # TODO: maybe save the model after evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
