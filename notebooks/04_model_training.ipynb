{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e8e6db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:38:28,989 - api.services.database_service - INFO - Connected to MongoDB database: healthcare\n",
      "2025-07-25 18:38:28,993 - src.utils.notebook_setup - INFO - Database connected: True\n",
      "2025-07-25 18:38:28,994 - src.utils.notebook_setup - INFO - Database collections: ['heart_disease_silver', 'heart_disease_bronze', 'heart_disease_gold']\n",
      "2025-07-25 18:38:28,995 - src.utils.notebook_setup - INFO - Database collections count: 3\n",
      "2025-07-25 18:38:28,996 - src.utils.notebook_setup - INFO - === STARTING MODEL TRAINING ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>thalch</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>cp_asymptomatic</th>\n",
       "      <th>cp_atypical angina</th>\n",
       "      <th>cp_non-anginal</th>\n",
       "      <th>cp_typical angina</th>\n",
       "      <th>restecg_normal</th>\n",
       "      <th>slope_flat</th>\n",
       "      <th>slope_not_tested</th>\n",
       "      <th>slope_upsloping</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.795918</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.388031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338028</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.795918</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.277992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.485915</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.183673</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.318533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.229730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.291506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.353282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.725352</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.326255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.510204</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.227799</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.669014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.647727</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex  trestbps      chol  fbs    thalch  exang   oldpeak  \\\n",
       "0  0.714286  1.0  0.541667  0.285714  1.0  0.633803    0.0  0.556818   \n",
       "1  0.795918  1.0  0.666667  0.388031  0.0  0.338028    1.0  0.465909   \n",
       "2  0.795918  1.0  0.333333  0.277992  0.0  0.485915    1.0  0.590909   \n",
       "3  0.183673  1.0  0.416667  0.318533  0.0  0.894366    0.0  0.693182   \n",
       "4  0.265306  0.0  0.416667  0.229730  0.0  0.788732    0.0  0.454545   \n",
       "5  0.571429  1.0  0.333333  0.291506  0.0  0.830986    0.0  0.386364   \n",
       "6  0.693878  0.0  0.500000  0.353282  0.0  0.704225    0.0  0.704545   \n",
       "7  0.591837  0.0  0.333333  0.519305  0.0  0.725352    1.0  0.363636   \n",
       "8  0.714286  1.0  0.416667  0.326255  0.0  0.612676    0.0  0.454545   \n",
       "9  0.510204  1.0  0.500000  0.227799  1.0  0.669014    1.0  0.647727   \n",
       "\n",
       "   cp_asymptomatic  cp_atypical angina  cp_non-anginal  cp_typical angina  \\\n",
       "0              0.0                 0.0             0.0                1.0   \n",
       "1              1.0                 0.0             0.0                0.0   \n",
       "2              1.0                 0.0             0.0                0.0   \n",
       "3              0.0                 0.0             1.0                0.0   \n",
       "4              0.0                 1.0             0.0                0.0   \n",
       "5              0.0                 1.0             0.0                0.0   \n",
       "6              1.0                 0.0             0.0                0.0   \n",
       "7              1.0                 0.0             0.0                0.0   \n",
       "8              1.0                 0.0             0.0                0.0   \n",
       "9              1.0                 0.0             0.0                0.0   \n",
       "\n",
       "   restecg_normal  slope_flat  slope_not_tested  slope_upsloping  target  \n",
       "0             0.0         0.0               0.0              0.0       0  \n",
       "1             0.0         1.0               0.0              0.0       1  \n",
       "2             0.0         1.0               0.0              0.0       1  \n",
       "3             1.0         0.0               0.0              0.0       0  \n",
       "4             0.0         0.0               0.0              1.0       0  \n",
       "5             1.0         0.0               0.0              1.0       0  \n",
       "6             0.0         0.0               0.0              0.0       1  \n",
       "7             1.0         0.0               0.0              1.0       0  \n",
       "8             0.0         1.0               0.0              0.0       1  \n",
       "9             0.0         0.0               0.0              0.0       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from src.utils.data_utils import drop_id\n",
    "from src.utils.training_utils import (\n",
    "    prepare_data,\n",
    "    train_model,\n",
    "    hyperparameter_tuning,\n",
    "    save_model,\n",
    "    remove_old_models,\n",
    "    get_models,\n",
    ")\n",
    "from src.utils.notebook_setup import setup_notebook_environment\n",
    "\n",
    "# Quick setup\n",
    "dbs, logger = await setup_notebook_environment()\n",
    "\n",
    "# Now ready to work\n",
    "logger.info(\"=== STARTING MODEL TRAINING ===\")\n",
    "\n",
    "# Checking Gold Layer\n",
    "gold_data_from_db = await dbs.get_gold_data()\n",
    "gold_data_df = pd.DataFrame(gold_data_from_db)\n",
    "gold_data_df = drop_id(gold_data_df)\n",
    "\n",
    "gold_data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be59a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:38:29,064 - src.utils.training_utils - INFO - Training set: (1472, 16), Test set: (368, 16)\n",
      "2025-07-25 18:38:29,065 - src.utils.notebook_setup - INFO - X_train shape: (1472, 16)\n",
      "2025-07-25 18:38:29,066 - src.utils.notebook_setup - INFO - X_test shape: (368, 16)\n",
      "2025-07-25 18:38:29,067 - src.utils.notebook_setup - INFO - y_train shape: (1472,)\n",
      "2025-07-25 18:38:29,068 - src.utils.notebook_setup - INFO - y_test shape: (368,)\n"
     ]
    }
   ],
   "source": [
    "# SEPARATE features and target\n",
    "X = gold_data_df.drop('target', axis=1)  # Features only (18 columns)\n",
    "y = gold_data_df['target']               # Target only (0s and 1s)\n",
    "\n",
    "# Then do train/test split\n",
    "X_train, X_test, y_train, y_test = prepare_data(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "logger.info(f\"X_train shape: {X_train.shape}\")\n",
    "logger.info(f\"X_test shape: {X_test.shape}\")\n",
    "logger.info(f\"y_train shape: {y_train.shape}\")\n",
    "logger.info(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e6eeb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:38:29,076 - src.utils.training_utils - INFO - \n",
      "Retrieved models: dict_keys(['LogisticRegression', 'RandomForest', 'XGBoost', 'DecisionTree', 'Naive Bayes', 'AdaBoost', 'Gradient Boosting', 'SVM'])\n",
      "2025-07-25 18:38:29,091 - src.utils.notebook_setup - INFO - None\n",
      "2025-07-25 18:38:29,093 - src.utils.notebook_setup - INFO - None\n",
      "2025-07-25 18:38:29,094 - src.utils.notebook_setup - INFO - None\n",
      "2025-07-25 18:38:29,095 - src.utils.notebook_setup - INFO - None\n",
      "2025-07-25 18:38:29,096 - src.utils.notebook_setup - INFO - None\n",
      "2025-07-25 18:38:29,098 - src.utils.notebook_setup - INFO - None\n",
      "2025-07-25 18:38:29,099 - src.utils.notebook_setup - INFO - None\n",
      "2025-07-25 18:38:29,100 - src.utils.notebook_setup - INFO - None\n",
      "2025-07-25 18:38:29,101 - src.utils.notebook_setup - INFO - None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'AdaBoost': AdaBoostClassifier(random_state=42),\n",
      "    'DecisionTree': DecisionTreeClassifier(class_weight='balanced', max_depth=5, random_state=42),\n",
      "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
      "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
      "    'Naive Bayes': GaussianNB(),\n",
      "    'RandomForest': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
      "    'SVM': SVC(C=1, probability=True),\n",
      "    'XGBoost': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...)}\n",
      "{'AdaBoost': AdaBoostClassifier(random_state=42),\n",
      " 'DecisionTree': DecisionTreeClassifier(class_weight='balanced', max_depth=5, random_state=42),\n",
      " 'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
      " 'LogisticRegression': LogisticRegression(max_iter=1000),\n",
      " 'Naive Bayes': GaussianNB(),\n",
      " 'RandomForest': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
      " 'SVM': SVC(C=1, probability=True),\n",
      " 'XGBoost': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...)}\n",
      "{'AdaBoost': AdaBoostClassifier(random_state=42),\n",
      " 'DecisionTree': DecisionTreeClassifier(class_weight='balanced', max_depth=5, random_state=42),\n",
      " 'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
      " 'LogisticRegression': LogisticRegression(max_iter=1000),\n",
      " 'Naive Bayes': GaussianNB(),\n",
      " 'RandomForest': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
      " 'SVM': SVC(C=1, probability=True),\n",
      " 'XGBoost': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...)}\n",
      "(\" LogisticRegression Parameters: {'C': 1.0, 'class_weight': None, 'dual': \"\n",
      " \"False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, \"\n",
      " \"'max_iter': 1000, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', \"\n",
      " \"'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, \"\n",
      " \"'warm_start': False}\")\n",
      "(\" RandomForest Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, \"\n",
      " \"'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': None, \"\n",
      " \"'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, \"\n",
      " \"'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, \"\n",
      " \"'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, \"\n",
      " \"'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\")\n",
      "(\" XGBoost Parameters: {'objective': 'binary:logistic', 'base_score': None, \"\n",
      " \"'booster': None, 'callbacks': None, 'colsample_bylevel': None, \"\n",
      " \"'colsample_bynode': None, 'colsample_bytree': None, 'device': None, \"\n",
      " \"'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': \"\n",
      " \"None, 'feature_types': None, 'gamma': None, 'grow_policy': None, \"\n",
      " \"'importance_type': None, 'interaction_constraints': None, 'learning_rate': \"\n",
      " \"0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, \"\n",
      " \"'max_delta_step': None, 'max_depth': None, 'max_leaves': None, \"\n",
      " \"'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, \"\n",
      " \"'multi_strategy': None, 'n_estimators': 100, 'n_jobs': None, \"\n",
      " \"'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, \"\n",
      " \"'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': \"\n",
      " \"0.8083538083538083, 'subsample': None, 'tree_method': None, \"\n",
      " \"'validate_parameters': None, 'verbosity': None}\")\n",
      "(\" DecisionTree Parameters: {'ccp_alpha': 0.0, 'class_weight': 'balanced', \"\n",
      " \"'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': \"\n",
      " \"None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, \"\n",
      " \"'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 42, \"\n",
      " \"'splitter': 'best'}\")\n",
      "\" Naive Bayes Parameters: {'priors': None, 'var_smoothing': 1e-09}\"\n",
      "(\" AdaBoost Parameters: {'algorithm': 'SAMME.R', 'base_estimator': \"\n",
      " \"'deprecated', 'estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, \"\n",
      " \"'random_state': 42}\")\n",
      "(\" Gradient Boosting Parameters: {'ccp_alpha': 0.0, 'criterion': \"\n",
      " \"'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', \"\n",
      " \"'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, \"\n",
      " \"'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, \"\n",
      " \"'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': \"\n",
      " \"None, 'random_state': 42, 'subsample': 1.0, 'tol': 0.0001, \"\n",
      " \"'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\")\n",
      "(\" SVM Parameters: {'C': 1, 'break_ties': False, 'cache_size': 200, \"\n",
      " \"'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', \"\n",
      " \"'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, \"\n",
      " \"'probability': True, 'random_state': None, 'shrinking': True, 'tol': 0.001, \"\n",
      " \"'verbose': False}\")\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "models  = get_models(y_train)\n",
    "\n",
    "pprint(models, indent=4) # Use 4 spaces for inden\n",
    "pprint(models, width=40) # Set max line width\n",
    "\n",
    "logger.info(pprint(models))\n",
    "\n",
    "# Shows ALL available parameters\n",
    "\n",
    "for model in models:\n",
    "    logger.info(pprint(f\" {model} Parameters: {models[model].get_params()}\") ) \n",
    "# logger.info(pprint(f\" Random Forest Parameters: {models['RandomForest'].get_params()}\"))\n",
    "# logger.info(pprint(f\" XGBoost Parameters: {models['XGBoost'].get_params()}\"))\n",
    "# logger.info(pprint(f\" Decision Tree Parameters: {models['DecisionTree'].get_params()}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d35bb93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:38:29,114 - src.utils.training_utils - INFO - Removing old model: heart_disease_classifier.joblib\n",
      "2025-07-25 18:38:29,115 - src.utils.notebook_setup - INFO - Training LogisticRegression...\n",
      "2025-07-25 18:38:29,135 - src.utils.training_utils - INFO - Trained LogisticRegression\n",
      "2025-07-25 18:38:29,136 - src.utils.notebook_setup - INFO - Tuning hyperparameters for LogisticRegression...\n",
      "2025-07-25 18:38:34,954 - src.utils.training_utils - INFO - Hyperparameter tuning completed for LogisticRegression\n",
      "2025-07-25 18:38:34,957 - src.utils.notebook_setup - INFO - Training RandomForest...\n",
      "2025-07-25 18:38:35,180 - src.utils.training_utils - INFO - Trained RandomForest\n",
      "2025-07-25 18:38:35,181 - src.utils.notebook_setup - INFO - Tuning hyperparameters for RandomForest...\n",
      "2025-07-25 18:39:31,216 - src.utils.training_utils - INFO - Hyperparameter tuning completed for RandomForestClassifier\n",
      "2025-07-25 18:39:31,347 - src.utils.notebook_setup - INFO - Training XGBoost...\n",
      "2025-07-25 18:39:31,760 - src.utils.training_utils - INFO - Trained XGBoost\n",
      "2025-07-25 18:39:31,763 - src.utils.notebook_setup - INFO - Tuning hyperparameters for XGBoost...\n",
      "2025-07-25 18:39:49,364 - src.utils.training_utils - INFO - Hyperparameter tuning completed for XGBClassifier\n",
      "2025-07-25 18:39:49,384 - src.utils.notebook_setup - INFO - Training DecisionTree...\n",
      "2025-07-25 18:39:49,401 - src.utils.training_utils - INFO - Trained DecisionTree\n",
      "2025-07-25 18:39:49,404 - src.utils.notebook_setup - INFO - Tuning hyperparameters for DecisionTree...\n",
      "2025-07-25 18:39:50,472 - src.utils.training_utils - INFO - Hyperparameter tuning completed for DecisionTreeClassifier\n",
      "2025-07-25 18:39:50,480 - src.utils.notebook_setup - INFO - Training Naive Bayes...\n",
      "2025-07-25 18:39:50,490 - src.utils.training_utils - INFO - Trained Naive Bayes\n",
      "2025-07-25 18:39:50,493 - src.utils.notebook_setup - INFO - Tuning hyperparameters for Naive Bayes...\n",
      "2025-07-25 18:39:50,609 - src.utils.training_utils - INFO - Hyperparameter tuning completed for GaussianNB\n",
      "2025-07-25 18:39:50,614 - src.utils.notebook_setup - INFO - Training AdaBoost...\n",
      "2025-07-25 18:39:50,849 - src.utils.training_utils - INFO - Trained AdaBoost\n",
      "2025-07-25 18:39:50,851 - src.utils.notebook_setup - INFO - Tuning hyperparameters for AdaBoost...\n",
      "2025-07-25 18:39:55,703 - src.utils.training_utils - INFO - Hyperparameter tuning completed for AdaBoostClassifier\n",
      "2025-07-25 18:39:55,903 - src.utils.notebook_setup - INFO - Training Gradient Boosting...\n",
      "2025-07-25 18:39:56,273 - src.utils.training_utils - INFO - Trained Gradient Boosting\n",
      "2025-07-25 18:39:56,275 - src.utils.notebook_setup - INFO - Tuning hyperparameters for Gradient Boosting...\n",
      "2025-07-25 18:40:09,608 - src.utils.training_utils - INFO - Hyperparameter tuning completed for GradientBoostingClassifier\n",
      "2025-07-25 18:40:09,642 - src.utils.notebook_setup - INFO - Training SVM...\n",
      "2025-07-25 18:40:10,133 - src.utils.training_utils - INFO - Trained SVM\n",
      "2025-07-25 18:40:10,135 - src.utils.notebook_setup - INFO - Tuning hyperparameters for SVM...\n",
      "2025-07-25 18:40:13,238 - src.utils.training_utils - INFO - Hyperparameter tuning completed for SVC\n"
     ]
    }
   ],
   "source": [
    "# train the models\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'max_depth': [3, 5, 10, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        # GaussianNB has no hyperparams that typically require tuning\n",
    "        'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 1]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "remove_old_models()\n",
    "# Train models with correct parameter grids\n",
    "for name, model in models.items():\n",
    "    logger.info(f\"Training {name}...\")\n",
    "    \n",
    "    # Train basic model first\n",
    "    model = train_model(model, X_train, y_train, name)\n",
    "    \n",
    "    # Get the correct parameter grid for this model\n",
    "    if name in param_grids:\n",
    "        param_grid = param_grids[name]\n",
    "        logger.info(f\"Tuning hyperparameters for {name}...\")\n",
    "        model = hyperparameter_tuning(model, X_train, y_train, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    else:\n",
    "        logger.info(f\"No hyperparameter tuning for {name}\")\n",
    "    \n",
    "    # Save the model\n",
    "    save_model(model, name) # TODO: maybe save the model after evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
